<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Justin Le https://blog.jle.im (justin@jle.im)">
  <title>Practical Dependent Types: Type-Safe Neural Networks</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/beige.css" id="theme">
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section>
  <h1 class="title">Practical Dependent Types: Type-Safe Neural Networks</h1>
  <p class="author">Justin Le https://blog.jle.im (justin@jle.im)</p>
  <p class="date">Kiev Functional Programming, Aug 16, 2017</p>
</section>

<section id="preface" class="slide level2">
<h2>Preface</h2>
<p>Slide available at <a href="https://mstksg.github.io/talks/kievfprog/dependent-types.html" class="uri">https://mstksg.github.io/talks/kievfprog/dependent-types.html</a>.</p>
<p>All code available at <a href="https://github.com/mstksg/talks/tree/master/kievfprog" class="uri">https://github.com/mstksg/talks/tree/master/kievfprog</a>.</p>
<p>Libraries required: (available on Hackage) <em>hmatrix</em>, <em>singletons</em>, <em>MonadRandom</em>. GHC 8.x assumed.</p>
</section>
<section id="the-big-question" class="slide level2">
<h2>The Big Question</h2>
<p>The big question of Haskell: <em>What can types do for us?</em></p>
<div class="fragment">
<p>Dependent types are simply the extension of this question, pushing the power of types further.</p>
</div>
</section>
<section id="artificial-neural-networks" class="slide level2">
<h2>Artificial Neural Networks</h2>
<figure>
<img src="img/ffneural.png" title="Feed-forward ANN architecture" alt="Feed-forward ANN architecture" /><figcaption>Feed-forward ANN architecture</figcaption>
</figure>
</section>
<section id="parameterized-functions" class="slide level2">
<h2>Parameterized functions</h2>
<p>Each layer receives an input vector, <span class="math inline"><strong>x</strong> : ℝ<sup><em>n</em></sup></span>, and produces an output <span class="math inline"><strong>y</strong> : ℝ<sup><em>m</em></sup></span>.</p>
<div class="fragment">
<p>They are parameterized by a weight matrix <span class="math inline"><em>W</em> : ℝ<sup><em>m</em> × <em>n</em></sup></span> (an <span class="math inline"><em>m</em> × <em>n</em></span> matrix) and a bias vector <span class="math inline"><strong>b</strong> : ℝ<sup><em>m</em></sup></span>, and the result is: (for some activation function <code>f</code>)</p>
<p><br /><span class="math display"><strong>y</strong> = <em>f</em>(<em>W</em><strong>x</strong> + <strong>b</strong>)</span><br /></p>
<p>A neural network would take a vector through many layers.</p>
</div>
</section>
<section id="networks-in-haskell" class="slide level2">
<h2>Networks in Haskell</h2>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">data</span> <span class="dt">Weights</span> <span class="fu">=</span> <span class="dt">W</span> {<span class="ot"> wBiases ::</span> <span class="fu">!</span>(<span class="dt">Vector</span> <span class="dt">Double</span>)  <span class="co">-- n</span>
                 ,<span class="ot"> wNodes  ::</span> <span class="fu">!</span>(<span class="dt">Matrix</span> <span class="dt">Double</span>)  <span class="co">-- n x m</span>
                 }                              <span class="co">-- &quot;m to n&quot; layer</span>


<span class="kw">data</span> <span class="dt">Network</span><span class="ot"> ::</span> <span class="dt">Type</span> <span class="kw">where</span>
    <span class="dt">O</span><span class="ot">    ::</span> <span class="fu">!</span><span class="dt">Weights</span> <span class="ot">-&gt;</span> <span class="dt">Network</span>
<span class="ot">    (:~) ::</span> <span class="fu">!</span><span class="dt">Weights</span> <span class="ot">-&gt;</span> <span class="fu">!</span><span class="dt">Network</span> <span class="ot">-&gt;</span> <span class="dt">Network</span>
<span class="kw">infixr</span> <span class="dv">5</span> <span class="fu">:~</span></code></pre></div>
<div class="fragment">
<p>A network with one input layer, two hidden layers, and one output layer would be:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell">h1 <span class="fu">:~</span> h2 <span class="fu">:~</span> <span class="dt">O</span> o</code></pre></div>
</div>
</section>
<section id="running-them" class="slide level2">
<h2>Running them</h2>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">runLayer ::</span> <span class="dt">Weights</span> <span class="ot">-&gt;</span> <span class="dt">Vector</span> <span class="dt">Double</span> <span class="ot">-&gt;</span> <span class="dt">Vector</span> <span class="dt">Double</span>
runLayer (<span class="dt">W</span> wB wN) v <span class="fu">=</span> wB <span class="fu">+</span> wN <span class="fu">#&gt;</span> v

<span class="ot">runNet ::</span> <span class="dt">Network</span> <span class="ot">-&gt;</span> <span class="dt">Vector</span> <span class="dt">Double</span> <span class="ot">-&gt;</span> <span class="dt">Vector</span> <span class="dt">Double</span>
runNet (<span class="dt">O</span> w)     <span class="fu">!</span>v <span class="fu">=</span> logistic (runLayer w v)
runNet (w <span class="fu">:~</span> n&#39;) <span class="fu">!</span>v <span class="fu">=</span> <span class="kw">let</span> v&#39; <span class="fu">=</span> logistic (runLayer w v)
                      <span class="kw">in</span>  runNet n&#39; v&#39;</code></pre></div>
</section>
<section id="generating-them" class="slide level2">
<h2>Generating them</h2>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">randomWeights ::</span> <span class="dt">MonadRandom</span> m <span class="ot">=&gt;</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> m <span class="dt">Weights</span>
randomWeights i o <span class="fu">=</span> <span class="kw">do</span>
<span class="ot">    seed1 ::</span> <span class="dt">Int</span> <span class="ot">&lt;-</span> getRandom
<span class="ot">    seed2 ::</span> <span class="dt">Int</span> <span class="ot">&lt;-</span> getRandom
    <span class="kw">let</span> wB <span class="fu">=</span> randomVector  seed1 <span class="dt">Uniform</span> o <span class="fu">*</span> <span class="dv">2</span> <span class="fu">-</span> <span class="dv">1</span>
        wN <span class="fu">=</span> uniformSample seed2 o (replicate i (<span class="fu">-</span><span class="dv">1</span>, <span class="dv">1</span>))
    return <span class="fu">$</span> <span class="dt">W</span> wB wN

<span class="ot">randomNet ::</span> <span class="dt">MonadRandom</span> m <span class="ot">=&gt;</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> [<span class="dt">Int</span>] <span class="ot">-&gt;</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> m <span class="dt">Network</span>
randomNet i []     o <span class="fu">=</span>    <span class="dt">O</span> <span class="fu">&lt;$&gt;</span> randomWeights i o
randomNet i (h<span class="fu">:</span>hs) o <span class="fu">=</span> (<span class="fu">:~</span>) <span class="fu">&lt;$&gt;</span> randomWeights i h <span class="fu">&lt;*&gt;</span> randomNet h hs o</code></pre></div>
</section>
<section id="haskell-heart-attacks" class="slide level2">
<h2>Haskell Heart Attacks</h2>
<ul>
<li class="fragment">What if we mixed up the dimensions for <code>randomWeights</code>?</li>
<li class="fragment">What if the <em>user</em> mixed up the dimensions for <code>randomWeights</code>?</li>
<li class="fragment">What if layers in the network are incompatible?</li>
<li class="fragment">How does the user know what size vector a network expects?</li>
<li class="fragment">Is our <code>runLayer</code> and <code>runNet</code> implementation correct?</li>
</ul>
</section>
<section id="backprop" class="slide level2">
<h2>Backprop</h2>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">train ::</span> <span class="dt">Double</span>           <span class="co">-- ^ learning rate</span>
      <span class="ot">-&gt;</span> <span class="dt">Vector</span> <span class="dt">Double</span>    <span class="co">-- ^ input vector</span>
      <span class="ot">-&gt;</span> <span class="dt">Vector</span> <span class="dt">Double</span>    <span class="co">-- ^ target vector</span>
      <span class="ot">-&gt;</span> <span class="dt">Network</span>          <span class="co">-- ^ network to train</span>
      <span class="ot">-&gt;</span> <span class="dt">Network</span>
train rate x0 target <span class="fu">=</span> fst <span class="fu">.</span> go x0
  <span class="kw">where</span></code></pre></div>
</section>
<section id="backprop-outer-layer" class="slide level2">
<h2>Backprop (Outer layer)</h2>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">    go ::</span> <span class="dt">Vector</span> <span class="dt">Double</span>    <span class="co">-- ^ input vector</span>
       <span class="ot">-&gt;</span> <span class="dt">Network</span>          <span class="co">-- ^ network to train</span>
       <span class="ot">-&gt;</span> (<span class="dt">Network</span>, <span class="dt">Vector</span> <span class="dt">Double</span>)
    <span class="co">-- handle the output layer</span>
    go <span class="fu">!</span>x (<span class="dt">O</span> w<span class="fu">@</span>(<span class="dt">W</span> wB wN))
        <span class="fu">=</span> <span class="kw">let</span> y    <span class="fu">=</span> runLayer w x
              o    <span class="fu">=</span> logistic y
              <span class="co">-- the gradient (how much y affects the error)</span>
              <span class="co">--   (logistic&#39; is the derivative of logistic)</span>
              dEdy <span class="fu">=</span> logistic&#39; y <span class="fu">*</span> (o <span class="fu">-</span> target)
              <span class="co">-- new bias weights and node weights</span>
              wB&#39;  <span class="fu">=</span> wB <span class="fu">-</span> scale rate dEdy
              wN&#39;  <span class="fu">=</span> wN <span class="fu">-</span> scale rate (dEdy <span class="ot">`outer`</span> x)
              w&#39;   <span class="fu">=</span> <span class="dt">W</span> wB&#39; wN&#39;
              <span class="co">-- bundle of derivatives for next step</span>
              dWs  <span class="fu">=</span> tr wN <span class="fu">#&gt;</span> dEdy
          <span class="kw">in</span>  (<span class="dt">O</span> w&#39;, dWs)</code></pre></div>
</section>
<section id="backprop-inner-layer" class="slide level2">
<h2>Backprop (Inner layer)</h2>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell">    <span class="co">-- handle the inner layers</span>
    go <span class="fu">!</span>x (w<span class="fu">@</span>(<span class="dt">W</span> wB wN) <span class="fu">:~</span> n)
        <span class="fu">=</span> <span class="kw">let</span> y          <span class="fu">=</span> runLayer w x
              o          <span class="fu">=</span> logistic y
              <span class="co">-- get dWs&#39;, bundle of derivatives from rest of the net</span>
              (n&#39;, dWs&#39;) <span class="fu">=</span> go o n
              <span class="co">-- the gradient (how much y affects the error)</span>
              dEdy       <span class="fu">=</span> logistic&#39; y <span class="fu">*</span> dWs&#39;
              <span class="co">-- new bias weights and node weights</span>
              wB&#39;  <span class="fu">=</span> wB <span class="fu">-</span> scale rate dEdy
              wN&#39;  <span class="fu">=</span> wN <span class="fu">-</span> scale rate (dEdy <span class="ot">`outer`</span> x)
              w&#39;   <span class="fu">=</span> <span class="dt">W</span> wB&#39; wN&#39;
              <span class="co">-- bundle of derivatives for next step</span>
              dWs  <span class="fu">=</span> tr wN <span class="fu">#&gt;</span> dEdy
          <span class="kw">in</span>  (w&#39; <span class="fu">:~</span> n&#39;, dWs)</code></pre></div>
</section>
<section id="compiler-o-where-art-thou" class="slide level2">
<h2>Compiler, O Where Art Thou?</h2>
<ul>
<li class="fragment">Haskell is all about the compiler helping guide you write your code. But how much did the compiler help there?</li>
<li class="fragment">How can the &quot;shape&quot; of the matrices guide our programming?</li>
<li class="fragment">We basically rely on naming conventions to make sure we write our code correctly.</li>
</ul>
</section>
<section id="haskell-red-flags" class="slide level2">
<h2>Haskell Red Flags</h2>
<ul>
<li class="fragment">How many ways can we write the function and have it still typecheck?</li>
<li class="fragment">How many of our functions are partial?</li>
</ul>
</section>
<section id="a-typed-alternative" class="slide level2">
<h2>A Typed Alternative</h2>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">data</span> <span class="dt">Weights</span> i o <span class="fu">=</span> <span class="dt">W</span> {<span class="ot"> wBiases ::</span> <span class="fu">!</span>(<span class="dt">R</span> o)
                     ,<span class="ot"> wNodes  ::</span> <span class="fu">!</span>(<span class="dt">L</span> o i)
                     }</code></pre></div>
<p>An <code>o x i</code> layer</p>
</section>
<section id="a-typed-alternative-1" class="slide level2">
<h2>A Typed Alternative</h2>
<p>From HMatrix:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="dt">R</span><span class="ot"> ::</span> <span class="dt">Nat</span> <span class="ot">-&gt;</span> <span class="dt">Type</span>
<span class="dt">L</span><span class="ot"> ::</span> <span class="dt">Nat</span> <span class="ot">-&gt;</span> <span class="dt">Nat</span> <span class="ot">-&gt;</span> <span class="dt">Type</span></code></pre></div>
<p>An <code>R 3</code> is a 3-vector, an <code>L 4 3</code> is a 4 x 3 matrix.</p>
<div class="fragment">
<p>Operations are typed:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">(+)  ::</span> <span class="dt">KnownNat</span> n               <span class="ot">=&gt;</span> <span class="dt">R</span> n   <span class="ot">-&gt;</span> <span class="dt">R</span> n <span class="ot">-&gt;</span> <span class="dt">R</span> n
<span class="ot">(&lt;#) ::</span> (<span class="dt">KnownNat</span> m, <span class="dt">KnownNat</span> n) <span class="ot">=&gt;</span> <span class="dt">L</span> m n <span class="ot">-&gt;</span> <span class="dt">R</span> n <span class="ot">-&gt;</span> <span class="dt">R</span> m</code></pre></div>
<p><code>KnownNat n</code> lets hmatrix use the <code>n</code> in the type. Typed holes can guide our development, too!</p>
</div>
</section>
<section id="data-kinds" class="slide level2">
<h2>Data Kinds</h2>
<p>With <code>-XDataKinds</code>, all values and types are lifted to types and kinds.</p>
<div class="fragment">
<p>In addition to the values <code>True</code>, <code>False</code>, and the type <code>Bool</code>, we also have the <strong>type</strong> <code>'True</code>, <code>'False</code>, and the <strong>kind</strong> <code>Bool</code>.</p>
<p>In addition to <code>:</code> and <code>[]</code> and the list type, we have <code>':</code> and <code>'[]</code> and the list kind.</p>
</div>
</section>
<section id="data-kinds-1" class="slide level2">
<h2>Data Kinds</h2>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell">ghci<span class="fu">&gt;</span> <span class="fu">:</span>t <span class="dt">True</span>
<span class="dt">Bool</span>
ghci<span class="fu">&gt;</span> <span class="fu">:</span>k <span class="ch">&#39;True</span>
<span class="dt">Bool</span>
ghci<span class="fu">&gt;</span> <span class="fu">:</span>t [<span class="dt">True</span>, <span class="dt">False</span>]
[<span class="dt">Bool</span>]
ghci<span class="fu">&gt;</span> <span class="fu">:</span>k <span class="ch">&#39;[ &#39;</span><span class="dt">True</span>, <span class="ch">&#39;False ]</span>
[<span class="dt">Bool</span>]</code></pre></div>
</section>
<section id="a-typed-alternative-2" class="slide level2">
<h2>A Typed Alternative</h2>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">data</span> <span class="dt">Network</span><span class="ot"> ::</span> <span class="dt">Nat</span> <span class="ot">-&gt;</span> [<span class="dt">Nat</span>] <span class="ot">-&gt;</span> <span class="dt">Nat</span> <span class="ot">-&gt;</span> <span class="dt">Type</span> <span class="kw">where</span>
    <span class="dt">O</span><span class="ot">    ::</span> <span class="fu">!</span>(<span class="dt">Weights</span> i o)
         <span class="ot">-&gt;</span> <span class="dt">Network</span> i <span class="ch">&#39;[] o</span>
<span class="ot">    (:~) ::</span> <span class="dt">KnownNat</span> h
         <span class="ot">=&gt;</span> <span class="fu">!</span>(<span class="dt">Weights</span> i h)
         <span class="ot">-&gt;</span> <span class="fu">!</span>(<span class="dt">Network</span> h hs o)
         <span class="ot">-&gt;</span> <span class="dt">Network</span> i (h <span class="ch">&#39;: hs) o</span>
<span class="kw">infixr</span> <span class="dv">5</span> <span class="fu">:~</span></code></pre></div>
<div class="fragment">
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">h1 ::</span> <span class="dt">Weight</span> <span class="dv">10</span> <span class="dv">8</span>
<span class="ot">h2 ::</span> <span class="dt">Weight</span> <span class="dv">8</span>  <span class="dv">5</span>
<span class="ot">o  ::</span> <span class="dt">Weight</span> <span class="dv">5</span>  <span class="dv">2</span>

            <span class="dt">O</span><span class="ot"> o ::</span> <span class="dt">Network</span> <span class="dv">5</span>  <span class="ch">&#39;[]     2</span>
      h2 <span class="fu">:~</span> <span class="dt">O</span><span class="ot"> o ::</span> <span class="dt">Network</span> <span class="dv">8</span>  <span class="ch">&#39;[5]    2</span>
h1 <span class="fu">:~</span> h2 <span class="fu">:~</span> <span class="dt">O</span><span class="ot"> o ::</span> <span class="dt">Network</span> <span class="dv">10</span> <span class="ch">&#39;[8, 5] 2</span>
h2 <span class="fu">:~</span> h1 <span class="fu">:~</span> <span class="dt">O</span> o <span class="co">-- type error</span></code></pre></div>
</div>
</section>
<section id="running" class="slide level2">
<h2>Running</h2>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">runLayer ::</span> (<span class="dt">KnownNat</span> i, <span class="dt">KnownNat</span> o)
         <span class="ot">=&gt;</span> <span class="dt">Weights</span> i o
         <span class="ot">-&gt;</span> <span class="dt">R</span> i
         <span class="ot">-&gt;</span> <span class="dt">R</span> o
runLayer (<span class="dt">W</span> wB wN) v <span class="fu">=</span> wB <span class="fu">+</span> wN <span class="fu">#&gt;</span> v

<span class="ot">runNet ::</span> (<span class="dt">KnownNat</span> i, <span class="dt">KnownNat</span> o)
       <span class="ot">=&gt;</span> <span class="dt">Network</span> i hs o
       <span class="ot">-&gt;</span> <span class="dt">R</span> i
       <span class="ot">-&gt;</span> <span class="dt">R</span> o
runNet (<span class="dt">O</span> w)     <span class="fu">!</span>v <span class="fu">=</span> logistic (runLayer w v)
runNet (w <span class="fu">:~</span> n&#39;) <span class="fu">!</span>v <span class="fu">=</span> <span class="kw">let</span> v&#39; <span class="fu">=</span> logistic (runLayer w v)
                      <span class="kw">in</span>  runNet n&#39; v&#39;</code></pre></div>
<p>Exactly the same! No loss in expressivity!</p>
</section>
<section id="running-1" class="slide level2">
<h2>Running</h2>
<p>Much better! Matrices and vector lengths are guaranteed to line up!</p>
<p>Also, note that the interface for <code>runNet</code> is better stated in its type. No need to reply on documentation.</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell">runNet
<span class="ot">    ::</span> (<span class="dt">KnownNat</span> i, <span class="dt">KnownNat</span> o)
    <span class="ot">=&gt;</span> <span class="dt">Network</span> i hs o <span class="ot">-&gt;</span> <span class="dt">R</span> i <span class="ot">-&gt;</span> <span class="dt">R</span> o</code></pre></div>
<p>The user knows that they have to pass in an <code>R i</code>, and knows to expect an <code>R o</code>.</p>
</section>
<section id="generating" class="slide level2">
<h2>Generating</h2>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">randomWeights ::</span> (<span class="dt">MonadRandom</span> m, <span class="dt">KnownNat</span> i, <span class="dt">KnownNat</span> o)
              <span class="ot">=&gt;</span> m (<span class="dt">Weights</span> i o)
randomWeights <span class="fu">=</span> <span class="kw">do</span>
<span class="ot">    s1 ::</span> <span class="dt">Int</span> <span class="ot">&lt;-</span> getRandom
<span class="ot">    s2 ::</span> <span class="dt">Int</span> <span class="ot">&lt;-</span> getRandom
    <span class="kw">let</span> wB <span class="fu">=</span> randomVector  s1 <span class="dt">Uniform</span> <span class="fu">*</span> <span class="dv">2</span> <span class="fu">-</span> <span class="dv">1</span>
        wN <span class="fu">=</span> uniformSample s2 (<span class="fu">-</span><span class="dv">1</span>) <span class="dv">1</span>
    return <span class="fu">$</span> <span class="dt">W</span> wB wN</code></pre></div>
<p>No need for explicit arguments! User can demand <code>i</code> and <code>o</code>. No reliance on documentation and parameter orders.</p>
</section>
<section id="generating-1" class="slide level2">
<h2>Generating</h2>
<p>But, for generating nets, we have a problem:</p>
<pre><code>randomNet :: forall m i hs o. (MonadRandom m, KnownNat i, KnownNat o)
          =&gt; m (Network i hs o)
randomNet = case hs of [] -&gt; ??</code></pre>
</section>
<section id="pattern-matching-on-types" class="slide level2">
<h2>Pattern matching on types</h2>
<p>The solution for pattern matching on types: singletons.</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="co">-- (not the actual impelentation)</span>

<span class="kw">data</span> <span class="dt">Sing</span><span class="ot"> ::</span> <span class="dt">Bool</span> <span class="ot">-&gt;</span> <span class="dt">Type</span> <span class="kw">where</span>
    <span class="dt">SFalse</span><span class="ot"> ::</span> <span class="dt">Sing</span> <span class="ch">&#39;False</span>
    <span class="dt">STrue</span><span class="ot">  ::</span> <span class="dt">Sing</span> <span class="ch">&#39;True</span>

<span class="kw">data</span> <span class="dt">Sing</span><span class="ot"> ::</span> [k] <span class="ot">-&gt;</span> <span class="dt">Type</span> <span class="kw">where</span>
    <span class="dt">SNil</span><span class="ot">  ::</span> <span class="dt">Sing</span> <span class="ch">&#39;[]</span>
    <span class="dt">SCons</span><span class="ot"> ::</span> <span class="dt">Sing</span> x <span class="ot">-&gt;</span> <span class="dt">Sing</span> xs <span class="ot">-&gt;</span> <span class="dt">Sing</span> (x <span class="ch">&#39;: xs)</span>

<span class="kw">data</span> <span class="dt">Sing</span><span class="ot"> ::</span> <span class="dt">Nat</span> <span class="ot">-&gt;</span> <span class="dt">Type</span> <span class="kw">where</span>
    <span class="dt">SNat</span><span class="ot"> ::</span> <span class="dt">KnownNat</span> n <span class="ot">=&gt;</span> <span class="dt">Sing</span> n</code></pre></div>
</section>
<section id="pattern-matching-on-types-1" class="slide level2">
<h2>Pattern matching on types</h2>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell">ghci<span class="fu">&gt;</span> <span class="fu">:</span>t <span class="dt">SFalse</span>
<span class="dt">Sing</span> <span class="ch">&#39;False</span>
ghci<span class="fu">&gt;</span> <span class="fu">:</span>t <span class="dt">STrue</span> <span class="ot">`SCons`</span> (<span class="dt">SFalse</span> <span class="ot">`SCons`</span> <span class="dt">SNil</span>)
<span class="dt">Sing</span> <span class="ch">&#39;[True, False]</span>
ghci<span class="fu">&gt;</span> <span class="fu">:</span>t <span class="dt">SNat</span> <span class="fu">@</span><span class="dv">1</span> <span class="ot">`SCons`</span> (<span class="dt">SNat</span> <span class="fu">@</span><span class="dv">2</span> <span class="ot">`SCons`</span> <span class="dt">SNil</span>)
<span class="dt">Sing</span> <span class="ch">&#39;[1, 2]</span></code></pre></div>
</section>
<section id="random-networks" class="slide level2">
<h2>Random networks</h2>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">randomNet&#39; ::</span> forall m i hs o<span class="fu">.</span> (<span class="dt">MonadRandom</span> m, <span class="dt">KnownNat</span> i, <span class="dt">KnownNat</span> o)
           <span class="ot">=&gt;</span> <span class="dt">Sing</span> hs <span class="ot">-&gt;</span> m (<span class="dt">Network</span> i hs o)
randomNet&#39; <span class="fu">=</span> \<span class="kw">case</span>
    <span class="dt">SNil</span>            <span class="ot">-&gt;</span>    <span class="dt">O</span> <span class="fu">&lt;$&gt;</span> randomWeights
    <span class="dt">SNat</span> <span class="ot">`SCons`</span> ss <span class="ot">-&gt;</span> (<span class="fu">:~</span>) <span class="fu">&lt;$&gt;</span> randomWeights <span class="fu">&lt;*&gt;</span> randomNet&#39; ss</code></pre></div>
</section>
<section id="implicit-passing" class="slide level2">
<h2>Implicit passing</h2>
<p>Explicitly passing singletons can be ugly.</p>
<div class="fragment">
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">class</span> <span class="dt">SingI</span> x <span class="kw">where</span>
<span class="ot">    sing ::</span> <span class="dt">Sing</span> x</code></pre></div>
<p>We can now recover the expressivity of the original function, and gain demand-driven shapes.</p>
</div>
</section>
<section id="implicit-passing-1" class="slide level2">
<h2>Implicit passing</h2>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">randomNet ::</span> forall i hs o m<span class="fu">.</span> (<span class="dt">MonadRandom</span> m, <span class="dt">KnownNat</span> i, <span class="dt">SingI</span> hs, <span class="dt">KnownNat</span> o)
          <span class="ot">=&gt;</span> m (<span class="dt">Network</span> i hs o)
randomNet <span class="fu">=</span> randomNet&#39; sing</code></pre></div>
<p>Now the shape can be inferred from the functions that use the <code>Network</code>.</p>
<div class="fragment">
<p>We can also demand them explicitly:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell">randomNet <span class="fu">@</span><span class="dv">1</span> <span class="fu">@</span><span class="ch">&#39;[8,5] @2</span></code></pre></div>
</div>
</section>
<section id="backprop-1" class="slide level2">
<h2>Backprop</h2>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">train ::</span> forall i hs o<span class="fu">.</span> (<span class="dt">KnownNat</span> i, <span class="dt">KnownNat</span> o)
      <span class="ot">=&gt;</span> <span class="dt">Double</span>           <span class="co">-- ^ learning rate</span>
      <span class="ot">-&gt;</span> <span class="dt">R</span> i              <span class="co">-- ^ input vector</span>
      <span class="ot">-&gt;</span> <span class="dt">R</span> o              <span class="co">-- ^ target vector</span>
      <span class="ot">-&gt;</span> <span class="dt">Network</span> i hs o   <span class="co">-- ^ network to train</span>
      <span class="ot">-&gt;</span> <span class="dt">Network</span> i hs o
train rate x0 target <span class="fu">=</span> fst <span class="fu">.</span> go x0</code></pre></div>
<div class="fragment">
<p>Ready for this?</p>
</div>
</section>
<section id="backprop-2" class="slide level2">
<h2>Backprop</h2>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">    go  ::</span> forall j js<span class="fu">.</span> <span class="dt">KnownNat</span> j
        <span class="ot">=&gt;</span> <span class="dt">R</span> j              <span class="co">-- ^ input vector</span>
        <span class="ot">-&gt;</span> <span class="dt">Network</span> j js o   <span class="co">-- ^ network to train</span>
        <span class="ot">-&gt;</span> (<span class="dt">Network</span> j js o, <span class="dt">R</span> j)
    <span class="co">-- handle the output layer</span>
    go <span class="fu">!</span>x (<span class="dt">O</span> w<span class="fu">@</span>(<span class="dt">W</span> wB wN))
        <span class="fu">=</span> <span class="kw">let</span> y    <span class="fu">=</span> runLayer w x
              o    <span class="fu">=</span> logistic y
              <span class="co">-- the gradient (how much y affects the error)</span>
              <span class="co">--   (logistic&#39; is the derivative of logistic)</span>
              dEdy <span class="fu">=</span> logistic&#39; y <span class="fu">*</span> (o <span class="fu">-</span> target)
              <span class="co">-- new bias weights and node weights</span>
              wB&#39;  <span class="fu">=</span> wB <span class="fu">-</span> konst rate <span class="fu">*</span> dEdy
              wN&#39;  <span class="fu">=</span> wN <span class="fu">-</span> konst rate <span class="fu">*</span> (dEdy <span class="ot">`outer`</span> x)
              w&#39;   <span class="fu">=</span> <span class="dt">W</span> wB&#39; wN&#39;
              <span class="co">-- bundle of derivatives for next step</span>
              dWs  <span class="fu">=</span> tr wN <span class="fu">#&gt;</span> dEdy
          <span class="kw">in</span>  (<span class="dt">O</span> w&#39;, dWs)
    <span class="co">-- handle the inner layers</span>
    go <span class="fu">!</span>x (w<span class="fu">@</span>(<span class="dt">W</span> wB wN) <span class="fu">:~</span> n)
        <span class="fu">=</span> <span class="kw">let</span> y          <span class="fu">=</span> runLayer w x
              o          <span class="fu">=</span> logistic y
              <span class="co">-- get dWs&#39;, bundle of derivatives from rest of the net</span>
              (n&#39;, dWs&#39;) <span class="fu">=</span> go o n
              <span class="co">-- the gradient (how much y affects the error)</span>
              dEdy       <span class="fu">=</span> logistic&#39; y <span class="fu">*</span> dWs&#39;
              <span class="co">-- new bias weights and node weights</span>
              wB&#39;  <span class="fu">=</span> wB <span class="fu">-</span> konst rate <span class="fu">*</span> dEdy
              wN&#39;  <span class="fu">=</span> wN <span class="fu">-</span> konst rate <span class="fu">*</span> (dEdy <span class="ot">`outer`</span> x)
              w&#39;   <span class="fu">=</span> <span class="dt">W</span> wB&#39; wN&#39;
              <span class="co">-- bundle of derivatives for next step</span>
              dWs  <span class="fu">=</span> tr wN <span class="fu">#&gt;</span> dEdy
          <span class="kw">in</span>  (w&#39; <span class="fu">:~</span> n&#39;, dWs)</code></pre></div>
</section>
<section id="backprop-3" class="slide level2">
<h2>Backprop</h2>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell">    <span class="co">-- handle the inner layers</span>
    go <span class="fu">!</span>x (w<span class="fu">@</span>(<span class="dt">W</span> wB wN) <span class="fu">:~</span> n)
        <span class="fu">=</span> <span class="kw">let</span> y          <span class="fu">=</span> runLayer w x
              o          <span class="fu">=</span> logistic y
              <span class="co">-- get dWs&#39;, bundle of derivatives from rest of the net</span>
              (n&#39;, dWs&#39;) <span class="fu">=</span> go o n
              <span class="co">-- the gradient (how much y affects the error)</span>
              dEdy       <span class="fu">=</span> logistic&#39; y <span class="fu">*</span> dWs&#39;
              <span class="co">-- new bias weights and node weights</span>
              wB&#39;  <span class="fu">=</span> wB <span class="fu">-</span> konst rate <span class="fu">*</span> dEdy
              wN&#39;  <span class="fu">=</span> wN <span class="fu">-</span> konst rate <span class="fu">*</span> (dEdy <span class="ot">`outer`</span> x)
              w&#39;   <span class="fu">=</span> <span class="dt">W</span> wB&#39; wN&#39;
              <span class="co">-- bundle of derivatives for next step</span>
              dWs  <span class="fu">=</span> tr wN <span class="fu">#&gt;</span> dEdy
          <span class="kw">in</span>  (w&#39; <span class="fu">:~</span> n&#39;, dWs)</code></pre></div>
<p>Surprise! It's actually <em>identical</em>! No loss in expressivity.</p>
<p>Also, typed holes can help you write your code in a lot of places. And shapes are all verified.</p>
<div class="fragment">
<p>By the way, still waiting for linear types in GHC :)</p>
</div>
</section>
<section id="type-driven-development" class="slide level2">
<h2>Type-Driven Development</h2>
<p>The overall guiding principle is:</p>
<ol type="1">
<li>Write an untyped implementation.</li>
<li>Realize where things can go wrong:
<ul>
<li>Partial functions?</li>
<li>Many, many ways to implement a function incorrectly with the current types?</li>
<li>Unclear or documentation-reliant API?</li>
</ul></li>
<li>Gradually add types in selective places to handle these.</li>
</ol>
<div class="fragment">
<p>I recommend not going the other way (use perfect type safety before figuring out where you actually really need them). We call that &quot;hasochism&quot;.</p>
</div>
</section>
<section id="further-reading" class="slide level2">
<h2>Further reading</h2>
<ul>
<li>Blog series: <a href="https://blog.jle.im/entries/series/+practical-dependent-types-in-haskell.html" class="uri">https://blog.jle.im/entries/series/+practical-dependent-types-in-haskell.html</a></li>
<li>Extra resources:</li>
<li><a href="https://www.youtube.com/watch?v=rhWMhTjQzsU" class="uri">https://www.youtube.com/watch?v=rhWMhTjQzsU</a></li>
<li><a href="http://www.well-typed.com/blog/2015/11/implementing-a-minimal-version-of-haskell-servant/" class="uri">http://www.well-typed.com/blog/2015/11/implementing-a-minimal-version-of-haskell-servant/</a></li>
<li><a href="https://www.schoolofhaskell.com/user/konn/prove-your-haskell-for-great-safety" class="uri">https://www.schoolofhaskell.com/user/konn/prove-your-haskell-for-great-safety</a></li>
<li><a href="http://jozefg.bitbucket.org/posts/2014-08-25-dep-types-part-1.html" class="uri">http://jozefg.bitbucket.org/posts/2014-08-25-dep-types-part-1.html</a></li>
</ul>
</section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
              { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
